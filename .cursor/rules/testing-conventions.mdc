---
description: Testing patterns and conventions for Go tests
globs: **/*_test.go
alwaysApply: false
---

# Testing Conventions

## Package Structure & Naming
- Use `package <module>_test` for behavior testing (not `package <module>`)  
- Use descriptive test names with "it" prefix: `"it parses successful response"`, `"it uses timestamp filtering for initial backfill request"`

## Test Organization & Structure

### Behavioral Grouping
- Group related tests by behavior: `TestServiceSyncBehavior`, `TestServiceObservabilityCallbacks`  
- Use `t.Run()` with descriptive behavior descriptions  
- Follow consistent naming: `func TestComponentBehaviorCategory(t *testing.T)`

### Test Method Naming Evolution
- **Match terminology to implementation**: Test names should reflect the public API, not internal concepts
- **Focus on what's being tested**: If testing `SyncDelegations()`, name tests around sync behavior
s
### Arrange-Act-Assert Pattern
- Always follow AAA structure with explicit comments:
```go
// Arrange
client := createMockClientWithResponses(data)

// Act  
result := service.DoSomething()

// Assert
assertExpectedBehavior(t, result)
```

## Parallel Execution
- Use `t.Parallel()` for **all** tests and sub-tests  
- Design tests to run concurrently without conflicts  
- Ensure resource isolation between parallel tests

## Interface Segregation for Testing
- **Prefer interface-based testing**: Use specific interfaces rather than full orchestrator methods
- **Test single concerns**: Call `SyncDelegations()` directly rather than `Run()` with timeouts
- **Interface segregation enables mocking**: Split responsibilities into focused interfaces:
  ```go
  // Example from scraper service refactoring
  type DelegationSyncer interface {
      SyncDelegations(ctx context.Context) error  // Core sync logic
  }
  type DelegationPoller interface {
      PollDelegations(ctx context.Context) error  // Polling with intervals  
  }
  type ServiceRunner interface {
      Run(ctx context.Context) error              // Orchestrator
  }
  ```
- **Single implementation, multiple interfaces**: One struct can implement all interfaces for production use while enabling focused testing
- **Mock specific interfaces**: Test each concern in isolation with targeted mocks
- **Test the right abstraction**: Test `SyncDelegations()` for business logic, not `Run()` for orchestration timing

## Deterministic Testing (Anti-Patterns to Avoid)
- **NEVER use timeout-based testing**: Avoid `context.WithTimeout()` for controlling test flow
- **NEVER use `time.Sleep()` in tests**: Use explicit synchronization instead
- **Avoid orchestrator testing for unit tests**: Test specific methods, not full service workflows
- **Bad example** (from scraper service before refactoring):
  ```go
  // BAD: Flaky, timing-dependent, tests orchestration not logic
  func runServiceBriefly(t *testing.T, service *scraper.Service) {
      ctx, cancel := context.WithTimeout(t.Context(), 50*time.Millisecond)
      defer cancel()
      err := service.Run(ctx) // Testing orchestrator timing, not business logic
      // Test passes/fails based on 50ms timing - FLAKY!
  }
  ```
- **Good example** (after refactoring):
  ```go
  // GOOD: Deterministic, tests specific behavior, immediate results
  ctx := t.Context()
  err := service.SyncDelegations(ctx) // Test core business logic directly
  require.NoError(t, err)
  // Test result is immediate and deterministic
  ```
- **Key insight**: We eliminated `runServiceBriefly()` entirely and tested `SyncDelegations()` directly, making tests 100% deterministic

## Helper Functions & Test Infrastructure

### Test Setup Helpers
- Factory functions: `createTestDelegation()`, `createMockClientWithResponses()`  
- Configuration builders: `createTestConfig()`, `createTestConfigWithChunkSize()`  
- Service builders: `createTestService(client, store, config)`

### Action Helpers
- Abstract common operations but avoid timeout-based helpers
- Mark every helper with `t.Helper()` if it takes `*testing.T`
- Handle context creation and cleanup properly

### Custom Assertion Helpers
- Domain-specific assertions: `assertFirstRequestUsesTimestampFilter()`  
- Descriptive names that express the business rule  
- Include helpful error messages  
- Use `t.Helper()` inside each assertion helper

## Assertion Strategy
- Use **`require`** for critical assertions that must halt the test  
- Use **`assert`** for non-critical checks that may continue execution  
- Always provide descriptive messages  
- Extract complex logic into helper assertions

## Mock & Test Data Management

### Mock Implementations
- Realistic mocks capturing request/response patterns  
- Track interactions for verification (e.g., `requests []tzkt.DelegationsRequest`)  
- Support multiple response scenarios via slices/queues

### Test Data Builders
- Factory functions create deterministic, realistic fixtures  
- Avoid hard-coding large structs inline; keep tests readable

### HTTP Test Infrastructure
- Use `httptest.NewServer()` to stub external APIs  
- Provide specialised handlers for success, error, malformed payloads  
- Always set correct headers and status codes

## File Structure Pattern
1. **Test functions** (grouped by behaviour)  
2. **Test setup helpers** (fixture builders, mocks)  
3. **Action helpers** (run services/handlers)  
4. **Assertion helpers** (verify behaviour)  
5. **Mock implementations** (bottom of file)

## Context & Resource Management
- **Use `t.Context()` for test contexts**: Provides proper test lifecycle management
- **NEVER use `context.Background()` in tests**: Loses test cancellation and cleanup
- **Avoid `context.WithTimeout()` for test control**: Use for bounded operations only
- Clean up resources with `defer server.Close()` / `defer cancel()`  
- Each test must own its resources—no shared state  
- Handle expected `context.Canceled` / `context.DeadlineExceeded` gracefully when testing cancellation

## Error Handling in Tests
- Cover both happy-path and failure scenarios  
- Verify specific error types with `errors.Is` / `assert.ErrorIs`  
- Provide custom helpers like `assertAPIError` for repetitive patterns

## Test Quality Principles
- **Deterministic** - no timeouts/sleeps; rely on explicit synchronisation  
- **Isolated** - tests can run in any order and in parallel  
- **Readable** - tests act as living documentation  
- **Maintainable** - helpers reduce duplication  
- **Comprehensive** - success & failure paths covered  
- **Fast** - minimised external IO, heavy use of `t.Parallel()`
- **Interface-focused** - test specific concerns, not orchestrators

## Test Execution
- Always run with the race detector and coverage:  
  `go test ./... -race -covermode=atomic -coverprofile=coverage.out`  
- Fail CI if overall coverage falls below **80%**  
- Upload coverage reports (e.g., Codecov) for visibility

## Behaviour Specification Style ("It" phrasing)
- Describe each `t.Run` with an English sentence starting with **"it"**:  
  `"it returns an error for invalid URL"`  
- Keep focus on observable behaviour, not implementation details  
- Align AAA comments with the "it" statement:  
  * **Arrange** - set up state  
  * **Act** - perform the action  
  * **Assert** - verify the expectation  
- Use `t.Run("it <behaviour>", ...)` consistently

## File & Directory Naming
- Unit tests: same directory, `<file>_test.go`  
- Integration/E2E tests: `integration/` or `e2e/`, tagged `//go:build integration`

## TDD Workflow Principles
1. **Red** - write a failing test that expresses the desired behaviour  
2. **Green** - implement just enough code to pass  
3. **Refactor** - improve code & tests without changing behaviour  
4. Commit once the suite is green

## Additional Best-Practice Notes
- Avoid testing unexported/private state; focus on public behaviour  
- Prefer dependency injection over globals for easier mocking  
- Each test should check one logical outcome  
- Minimise logging in tests; rely on assert/require messages  
- For concurrency, coordinate with `sync.WaitGroup` or channels—never `time.Sleep`  
- Test helpers must not contain production logic
- Use interface segregation to enable focused, deterministic testing
- Test specific interface methods rather than orchestrator workflows
